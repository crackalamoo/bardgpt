{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "os.chdir('drive')\n",
    "os.chdir('MyDrive')\n",
    "os.chdir('bardgpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Flatten, Dropout, Embedding,\\\n",
    "    Add, MultiHeadAttention, LayerNormalization, Input, Softmax\n",
    "\n",
    "from constants import *\n",
    "from tokens import pretty_tokens, rhymeMeterFromTokens\n",
    "\n",
    "OVERRIDE_CONSTANTS = False\n",
    "if OVERRIDE_CONSTANTS:\n",
    "    # change these to match the values used in data processing\n",
    "    MODEL_TYPE = 'b' # n=ngram, t=transformer, or b=bard\n",
    "    VOCAB_SIZE = 4096\n",
    "    NGRAM_N = 4\n",
    "    TRANSFORMER_N = 32\n",
    "    RHYME_STACK_SIZE = 4\n",
    "    METER_STACK_SIZE = 3\n",
    "\n",
    "EPOCHS = 10\n",
    "WARMUP_STEPS = 800\n",
    "EMBED_DIM = 512\n",
    "TRANSFORMER_LAYERS = 8\n",
    "TRANSFORMER_DFF = 1024\n",
    "RHYME_METER_DFF = 64\n",
    "TRANSFORMER_HEADS = 4\n",
    "VAL_SPLIT = 0.2\n",
    "SAVE_AT_END = False\n",
    "VERBOSE = False\n",
    "TRAINING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = NGRAM_N if MODEL_TYPE == 'n' else TRANSFORMER_N\n",
    "VOCAB = list(np.load('lemmas/lemmas.npy'))\n",
    "TEST_PROMPT = '<title> stop =ing by woods on a snowy evening <newline> '+\\\n",
    "    'whose woods these are i think i know <newline> '+\\\n",
    "    'his house is in the village though <newline> he'\n",
    "\n",
    "def sampleVocab(dist, temperature):\n",
    "    temperature = 1e-8 if temperature == 0 else temperature\n",
    "    dist = np.power(dist, temperature)\n",
    "    dist /= np.sum(dist)\n",
    "    sample = np.random.choice(np.arange(VOCAB_SIZE), p=dist)\n",
    "    return sample\n",
    "\n",
    "def genTokens(model, tokens, temperature=0.7, prompt=None):\n",
    "    res = [model.vocab.index(TITLE.lower()[1:-1])]\n",
    "    if prompt is not None:\n",
    "        res = [model.vocab.index(x) for x in prompt.split(' ') if x in model.vocab]\n",
    "    for _ in range(tokens):\n",
    "        pred = model.generate(res, temperature)\n",
    "        assert pred is not None\n",
    "        res.append(pred)\n",
    "    res = list(map(lambda token: model.vocab[token], res))\n",
    "    return res\n",
    "\n",
    "class LinearModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.vocab = VOCAB\n",
    "        self.seq = keras.Sequential([\n",
    "            Input(shape=(NGRAM_N-1, VOCAB_SIZE)),\n",
    "            Flatten(),\n",
    "            Dense(1024, activation='relu'),\n",
    "            Dense(1024, activation='relu'),\n",
    "            Dense(2048, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(VOCAB_SIZE, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    def call(self, input):\n",
    "        x = tf.one_hot(input, VOCAB_SIZE)\n",
    "        x = self.seq(x)\n",
    "        return x\n",
    "\n",
    "    def generate(self, fullContext, temperature=0.7):\n",
    "        context = fullContext[-(N-1):]\n",
    "        while len(context) > NGRAM_N-1:\n",
    "            context.pop(0)\n",
    "        while len(context) < NGRAM_N-1:\n",
    "            context.append(-1)\n",
    "        context = np.asarray([context])\n",
    "        pred = self.call(context)[0]\n",
    "        pred = sampleVocab(pred, temperature)\n",
    "        return pred\n",
    "\n",
    "\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth\n",
    "    angle_rates = 1 / (10000**depths)\n",
    "    angle_rads = positions * angle_rates\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class InputEmbedding(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = Embedding(input_dim=VOCAB_SIZE+1, output_dim=EMBED_DIM)\n",
    "        self.pos = positional_encoding(length=TRANSFORMER_N, depth=EMBED_DIM)\n",
    "        self.add = Add()\n",
    "        self.dropout = Dropout(0.1)\n",
    "    def call(self, input):\n",
    "        length = tf.shape(input)[1]\n",
    "        x = self.embed(input)\n",
    "        x *= tf.math.sqrt(tf.cast(EMBED_DIM, tf.float32))\n",
    "        x = self.add([x, self.pos[tf.newaxis, :length, :]])\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class AttentionBlock(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(**kwargs)\n",
    "        self.dropout = Dropout(0.1)\n",
    "        self.norm = LayerNormalization()\n",
    "        self.add = Add()\n",
    "    def call(self, input):\n",
    "        x = self.mha(query=input, value=input, key=input, use_causal_mask=True)\n",
    "        x = self.dropout(x)\n",
    "        x = self.add([input, x])\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class FeedForward(keras.layers.Layer):\n",
    "    def __init__(self, dff):\n",
    "        super().__init__()\n",
    "        self.seq = keras.Sequential([\n",
    "            Dense(dff, activation='relu'),\n",
    "            Dense(EMBED_DIM),\n",
    "            Dropout(0.1)\n",
    "        ])\n",
    "        self.add = Add()\n",
    "        self.norm = LayerNormalization()\n",
    "    def call(self, input):\n",
    "        x = self.add([input, self.seq(input)])\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, num_heads, dff):\n",
    "        super(Decoder, self).__init__()\n",
    "        attention = []\n",
    "        for i in range(num_layers):\n",
    "            attention.append(AttentionBlock(num_heads=num_heads, key_dim=EMBED_DIM, dropout=0.1))\n",
    "        self.attn_seq = keras.Sequential(attention)\n",
    "        self.ffn = FeedForward(dff)\n",
    "    def call(self, input):\n",
    "        x = self.attn_seq(input)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "class TransformerModel(keras.Model):\n",
    "    def __init__(self, *, num_layers=TRANSFORMER_LAYERS, num_heads=TRANSFORMER_HEADS, dff=TRANSFORMER_DFF):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.vocab = VOCAB\n",
    "        self.embed = InputEmbedding()\n",
    "        self.decoder = Decoder(num_layers=num_layers, num_heads=num_heads, dff=dff)\n",
    "        self.out = Dense(VOCAB_SIZE, activation='softmax')\n",
    "\n",
    "    def call(self, input):\n",
    "        x = self.embed(input) # context x embedding\n",
    "        x = self.decoder(x) # context x embedding\n",
    "        x = self.out(x) # context x vocab size\n",
    "        try:\n",
    "            del x._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        return x\n",
    "\n",
    "    def generate(self, fullContext, temperature=0.7):\n",
    "        context = fullContext[-N:]\n",
    "        lastToken = len(context)-1\n",
    "        while len(context) > TRANSFORMER_N:\n",
    "            context.pop(0)\n",
    "        while len(context) < TRANSFORMER_N:\n",
    "            context.append(-1)\n",
    "        context = np.asarray([context])+1\n",
    "        pred = self.call(context)[0]\n",
    "        pred = pred[lastToken]\n",
    "        pred = sampleVocab(pred, temperature)\n",
    "        return pred\n",
    "\n",
    "\n",
    "def rhyme_meter_encoding(input):\n",
    "    vowels = input[:,:,:RHYME_STACK_SIZE-1]\n",
    "    consonants = input[:,:,RHYME_STACK_SIZE-1:(RHYME_STACK_SIZE-1)*2]\n",
    "    rhyme_match = input[:,:,(RHYME_STACK_SIZE-1)*2:(RHYME_STACK_SIZE-1)*3]\n",
    "    vowels = tf.cast(vowels, tf.int8)\n",
    "    consonants = tf.cast(consonants, tf.int8)\n",
    "    vowels = tf.one_hot(vowels, depth=VOWEL_TYPES)\n",
    "    consonants = tf.one_hot(consonants, depth=CONSONANT_TYPES)\n",
    "    vowels = tf.reshape(vowels, shape=(tf.shape(vowels)[0], tf.shape(vowels)[1], -1))\n",
    "    consonants = tf.reshape(consonants, shape=(tf.shape(consonants)[0], tf.shape(consonants)[1], -1))\n",
    "    meter = input[:,:,-METER_STACK_SIZE:]\n",
    "    vowels = tf.cast(vowels, tf.float32)\n",
    "    consonants = tf.cast(consonants, tf.float32)\n",
    "    rhyme_match = tf.cast(rhyme_match, tf.float32)\n",
    "    meter = tf.cast(meter, tf.float32)\n",
    "    rhyme = tf.concat([vowels, consonants, rhyme_match], axis=2)\n",
    "    return rhyme, meter\n",
    "\n",
    "class RhymeMeterLayer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense_r1 = Dense(RHYME_METER_DFF, activation='relu')\n",
    "        self.dense_m1 = Dense(RHYME_METER_DFF//2, activation='relu')\n",
    "        self.dense_r2 = Dense(RHYME_METER_DFF, activation='relu')\n",
    "        # self.dense_m2 = Dense(RHYME_METER_DFF//2, activation='relu')\n",
    "        self.dense_3 = Dense(RHYME_METER_DFF*2, activation='relu')\n",
    "        self.dense_final = Dense(VOCAB_SIZE)\n",
    "    def call(self, input):\n",
    "        rhyme, meter = rhyme_meter_encoding(input)\n",
    "        rhyme = self.dense_r1(rhyme)\n",
    "        rhyme = self.dense_r2(rhyme)\n",
    "        meter = self.dense_m1(meter)\n",
    "        # meter = self.dense_m2(meter)\n",
    "        x = tf.concat([rhyme, meter], axis=2)\n",
    "        x = self.dense_3(x)\n",
    "        x = self.dense_final(x)\n",
    "        return x\n",
    "\n",
    "class BardModel(keras.Model):\n",
    "    def __init__(self, *, num_layers=TRANSFORMER_LAYERS, num_heads=TRANSFORMER_HEADS, dff=TRANSFORMER_DFF):\n",
    "        super(BardModel, self).__init__()\n",
    "        self.vocab = VOCAB\n",
    "        self.tl = VOCAB.index(TITLE.lower()[1:-1])\n",
    "        self.rhyme_types = max(VOWEL_TYPES, CONSONANT_TYPES)\n",
    "        self.embed = InputEmbedding()\n",
    "        self.decoder = Decoder(num_layers=num_layers, num_heads=num_heads, dff=dff)\n",
    "        self.transformer_pred = Dense(VOCAB_SIZE)\n",
    "        self.rhyme_meter_pred = RhymeMeterLayer()\n",
    "        self.add = Add()\n",
    "        self.softmax = Softmax()\n",
    "    \n",
    "    def call(self, input):\n",
    "        x = self.embed(input[0])\n",
    "        x = self.decoder(x)\n",
    "        x = self.transformer_pred(x)\n",
    "        try:\n",
    "            del x._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        rhyme_meter_x = self.rhyme_meter_pred(input[1])\n",
    "        x = self.add([x, rhyme_meter_x])\n",
    "        # x = rhyme_meter_x\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def generate(self, fullContext, temperature=0.7):\n",
    "        context = fullContext[-N:]\n",
    "        lastToken = len(context)-1\n",
    "        while len(context) > TRANSFORMER_N:\n",
    "            context.pop(0)\n",
    "        while len(context) < TRANSFORMER_N:\n",
    "            context.append(-1)\n",
    "        context = np.asarray([context])+1\n",
    "        rm = rhymeMeterFromTokens(fullContext, len(fullContext), self.tl, self.vocab)\n",
    "        rm = np.asarray([rm])\n",
    "        pred = self.call([context, rm])[0]\n",
    "        pred = pred[lastToken]\n",
    "        pred = sampleVocab(pred, temperature)\n",
    "        return pred\n",
    "\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=WARMUP_STEPS):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "def sparse_loss(y_true, y_pred):\n",
    "    loss_obj = keras.losses.SparseCategoricalCrossentropy(ignore_class=-1, reduction='none')\n",
    "    loss = loss_obj(y_true, y_pred)\n",
    "    return loss\n",
    "def sparse_perplexity(y_true, y_pred):\n",
    "    return tf.math.exp(tf.math.reduce_mean(sparse_loss(y_true, y_pred)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fname = {'n': 'inputs/ngram_train.npz',\n",
    "        't': 'inputs/transformer_train.npz',\n",
    "        'b': 'inputs/bard_train.npz'\n",
    "    }[MODEL_TYPE]\n",
    "    print(\"Loading data from\", fname)\n",
    "    loaded = np.load(fname)\n",
    "    train_x = loaded['x']\n",
    "    train_y = loaded['y']\n",
    "    if MODEL_TYPE == 'b':\n",
    "        train_x = [tf.convert_to_tensor(train_x), tf.convert_to_tensor(loaded['rm'])] # rhyme and syllables\n",
    "    if MODEL_TYPE == 'n':\n",
    "        train_x = tf.convert_to_tensor(train_x, tf.int32)\n",
    "    del loaded\n",
    "    \n",
    "    if TRAINING and VERBOSE:\n",
    "        if MODEL_TYPE != 'b':\n",
    "            print(\"X:\", train_x[10:14])\n",
    "        else:\n",
    "            print(\"X:\", train_x[0][10:14])\n",
    "            print(\"RM:\", train_x[1][10:14][1])\n",
    "        print(\"Y:\", train_y[10:14])\n",
    "        if MODEL_TYPE != 'b':\n",
    "            print(\"X shape:\", train_x.shape)\n",
    "        print(\"Y shape:\", train_y.shape)\n",
    "\n",
    "    print(\"Initializing model\")\n",
    "    models = {'n': LinearModel, 't': TransformerModel, 'b': BardModel}\n",
    "    model = models[MODEL_TYPE]()\n",
    "    if MODEL_TYPE != 'b':\n",
    "        res = model(train_x[:1])\n",
    "    else:\n",
    "        x0 = train_x[0][:1]\n",
    "        x1 = train_x[1][:1]\n",
    "        res = model([x0, x1])\n",
    "    if VERBOSE:\n",
    "        print(model)\n",
    "        print(res)\n",
    "    print(model.summary())\n",
    "\n",
    "    if TRAINING:\n",
    "        print(\"Compiling model\")\n",
    "        learning_rate = CustomSchedule(EMBED_DIM)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9),\n",
    "                    loss=sparse_loss, metrics=[sparse_perplexity])\n",
    "\n",
    "        print(\"Generating sample from baseline\")\n",
    "        print(pretty_tokens(genTokens(model, 25)))\n",
    "\n",
    "        print(\"Training model\")\n",
    "        min_perplexity = None\n",
    "        if not os.path.exists('saved_models'):\n",
    "            os.mkdir('saved_models')\n",
    "        class TrainCallback(keras.callbacks.Callback):\n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                global min_perplexity\n",
    "                val_perplexity = logs['val_sparse_perplexity']\n",
    "                print(\"\\rGenerating sample from model in training: \"+\n",
    "                    \"epoch \"+str(epoch+1)+\", perplexity \"+str(round(val_perplexity, 2)), end='')\n",
    "                print(pretty_tokens(genTokens(model, 75)))\n",
    "                if min_perplexity is None or val_perplexity <= min_perplexity:\n",
    "                    min_perplexity = val_perplexity\n",
    "                    print(\"Saving model\")\n",
    "                    model.save_weights('saved_models/'+MODEL_TYPE+'_model.h5') # no such file or directory right now\n",
    "\n",
    "        model.fit(train_x, train_y,\n",
    "                batch_size=256, validation_split=VAL_SPLIT, epochs=EPOCHS,\n",
    "                callbacks=[TrainCallback()])\n",
    "\n",
    "        if SAVE_AT_END:\n",
    "            print(\"Saving final model\")\n",
    "            model.save_weights('saved_models/'+MODEL_TYPE+'_model.h5')\n",
    "        \n",
    "        print(\"Generating samples from final model\")\n",
    "        if VERBOSE:\n",
    "            for i in range(10):\n",
    "                print(pretty_tokens(genTokens(model, 100)))\n",
    "            print(pretty_tokens(genTokens(model, 150, prompt=TEST_PROMPT)))\n",
    "            print(pretty_tokens(genTokens(model, 500)))\n",
    "        print(pretty_tokens(genTokens(model, 500)))\n",
    "    \n",
    "    else:\n",
    "        del train_x\n",
    "        del train_y\n",
    "        print(\"Loading weights\")\n",
    "        model.load_weights('saved_models/'+MODEL_TYPE+'_model.h5')\n",
    "\n",
    "        while True:\n",
    "            temp = 0.7\n",
    "            print(\"Commands:\\ng: generate sample with 250 tokens\\nl: generate sample with custom length\\np: generate sample with prompt\\nt: set temperature\\nq: quit\")\n",
    "            cmd = input(\"Enter command: \")\n",
    "            try:\n",
    "                if cmd == 'g':\n",
    "                    print(\"Generating sample...\")\n",
    "                    print(pretty_tokens(genTokens(model, 250, temperature=temp)))\n",
    "                if cmd == 'l':\n",
    "                    length = int(input(\"Enter length: \"))\n",
    "                    print(\"Generating sample...\")\n",
    "                    print(pretty_tokens(genTokens(model, length, temperature=temp)))\n",
    "                if cmd == 'p':\n",
    "                    prompt = \"\"\n",
    "                    print(\"Enter prompt as tokens separated by spaces and newlines.\")\n",
    "                    print(\"Example: <title> stop =ing by woods on a snowy evening\\nwhose woods these are i think i know\")\n",
    "                    print(\"All tokens not in the vocabulary will be ignored.\")\n",
    "                    while not prompt.endswith('\\n\\n\\n'):\n",
    "                        prompt += input(\"\")+'\\n'\n",
    "                    while prompt.startswith(' ') or prompt.startswith('\\n'):\n",
    "                        prompt = prompt[1:]\n",
    "                    while prompt.endswith(' ') or prompt.endswith('\\n'):\n",
    "                        prompt = prompt[:-1]\n",
    "                    prompt = prompt.replace('\\n', NEWLINE.lower())\n",
    "                    length = int(input(\"Enter length: \"))\n",
    "                    print(\"Generating sample...\")\n",
    "                    print(pretty_tokens(genTokens(model, length, temperature=temp, prompt=prompt)))\n",
    "                if cmd == 't':\n",
    "                    print(\"Current temperature:\", temp)\n",
    "                    temp = float(input(\"New temperature: \"))\n",
    "                    print(\"Temperature set to\", temp)\n",
    "                if cmd == 'q':\n",
    "                    sys.exit(0)\n",
    "            except Exception as e:\n",
    "                print(\"Error:\", e)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
